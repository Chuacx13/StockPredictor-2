{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 03:29:39.143436\n",
      "2004-08-15 03:29:39.143436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  ^GSPC_change  ^GSPC_lag\n",
      "0    2004-08-16     14.539917        NaN\n",
      "1    2004-08-17      2.369995  14.539917\n",
      "2    2004-08-18     13.460083   2.369995\n",
      "3    2004-08-19     -3.940063  13.460083\n",
      "4    2004-08-20      7.119995  -3.940063\n",
      "...         ...           ...        ...\n",
      "5029 2024-08-08     62.090332 -40.560059\n",
      "5030 2024-08-09     37.219727  62.090332\n",
      "5031 2024-08-12     25.100098  37.219727\n",
      "5032 2024-08-13     65.379883  25.100098\n",
      "5033 2024-08-14           NaN  65.379883\n",
      "\n",
      "[5034 rows x 3 columns]\n",
      "           Date  ^GSPC_change  ^GSPC_lag       ^IXIC        ^DJI      ^FCHI  \\\n",
      "0    2004-08-16     14.539917        NaN         NaN         NaN        NaN   \n",
      "1    2004-08-17      2.369995  14.539917   32.680054  130.150391  42.409912   \n",
      "2    2004-08-18     13.460083   2.369995   -4.599976    8.719727  15.120117   \n",
      "3    2004-08-19     -3.940063  13.460083   38.250000  118.560547  29.449951   \n",
      "4    2004-08-20      7.119995  -3.940063   -6.369995  -41.970703 -11.910156   \n",
      "...         ...           ...        ...         ...         ...        ...   \n",
      "5029 2024-08-08     62.090332 -40.560059 -214.041016 -289.710938  61.069824   \n",
      "5030 2024-08-09     37.219727  62.090332  228.250000  467.679688  36.690430   \n",
      "5031 2024-08-12     25.100098  37.219727  157.121094  147.953125  37.139648   \n",
      "5032 2024-08-13     65.379883  25.100098  151.099609 -110.742188 -19.139648   \n",
      "5033 2024-08-14           NaN  65.379883  282.859375  355.332031  52.459961   \n",
      "\n",
      "          ^GDAXI  \n",
      "0            NaN  \n",
      "1      53.129883  \n",
      "2       8.590088  \n",
      "3      25.070068  \n",
      "4     -13.270020  \n",
      "...          ...  \n",
      "5029   77.400391  \n",
      "5030  156.849609  \n",
      "5031  111.128906  \n",
      "5032   15.630859  \n",
      "5033    0.000000  \n",
      "\n",
      "[5034 rows x 7 columns]\n",
      "           Date  ^GSPC_change  ^GSPC_lag       ^IXIC        ^DJI      ^FCHI  \\\n",
      "0    2004-08-16     14.539917        NaN         NaN         NaN        NaN   \n",
      "1    2004-08-17      2.369995  14.539917   32.680054  130.150391  42.409912   \n",
      "2    2004-08-18     13.460083   2.369995   -4.599976    8.719727  15.120117   \n",
      "3    2004-08-19     -3.940063  13.460083   38.250000  118.560547  29.449951   \n",
      "4    2004-08-20      7.119995  -3.940063   -6.369995  -41.970703 -11.910156   \n",
      "...         ...           ...        ...         ...         ...        ...   \n",
      "5029 2024-08-08     62.090332 -40.560059 -214.041016 -289.710938  61.069824   \n",
      "5030 2024-08-09     37.219727  62.090332  228.250000  467.679688  36.690430   \n",
      "5031 2024-08-12     25.100098  37.219727  157.121094  147.953125  37.139648   \n",
      "5032 2024-08-13     65.379883  25.100098  151.099609 -110.742188 -19.139648   \n",
      "5033 2024-08-14           NaN  65.379883  282.859375  355.332031  52.459961   \n",
      "\n",
      "          ^GDAXI       ^AORD        ^HSI       ^N225       ^NSEI  \n",
      "0            NaN  -10.500000 -101.019531  -41.170898         NaN  \n",
      "1      53.129883    8.399902  -22.570312  -46.270508         NaN  \n",
      "2       8.590088   -7.399902   19.020508   49.439453         NaN  \n",
      "3      25.070068    8.000000  100.089844   41.610352         NaN  \n",
      "4     -13.270020    5.699951   18.340820   30.059570         NaN  \n",
      "...          ...         ...         ...         ...         ...  \n",
      "5029   77.400391  -26.600098  118.791016  185.359375 -131.550781  \n",
      "5030  156.849609  104.200195   -7.128906 -247.339844  -19.349609  \n",
      "5031  111.128906   37.799805    8.669922         NaN   26.949219  \n",
      "5032   15.630859   13.700195   13.849609  741.933594 -203.349609  \n",
      "5033    0.000000   28.000000 -120.320312  -93.171875  -40.650391  \n",
      "\n",
      "[5034 rows x 11 columns]\n",
      "(3406, 9)\n",
      "(791, 9)\n",
      "(3406, 1)\n",
      "(791, 1)\n",
      "      ^GSPC_lag     ^IXIC      ^DJI     ^FCHI    ^GDAXI     ^AORD      ^HSI  \\\n",
      "0     -0.020091  0.120217  0.077882 -0.279588 -0.162336 -0.449984 -1.572381   \n",
      "1     -0.394124 -0.056315 -0.232118 -1.761201 -0.325087 -1.267340  0.373283   \n",
      "2      2.007731  0.951006  1.733507  4.424063  1.393503  2.130730  0.245017   \n",
      "3      0.389447 -0.127495  0.351400  0.043538  0.814859  0.261408  0.329293   \n",
      "4     -0.504438  0.110415 -0.265657 -0.306056 -0.064611 -0.427276  0.851176   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "3401   0.124317  2.120837 -1.369944 -1.013274 -1.234244  0.660627  0.421722   \n",
      "3402   0.966128  1.355506  0.439025 -0.297345  0.241470 -0.168075 -2.144127   \n",
      "3403  -0.870432 -1.296369 -1.182265 -0.701542 -0.337097  0.507371 -2.181698   \n",
      "3404  -1.955585 -6.093708 -0.677804  0.153924  0.040695  0.155463  1.229487   \n",
      "3405   1.754906  2.019714  1.716939  1.138710  1.352812  0.757121  1.321142   \n",
      "\n",
      "         ^N225     ^NSEI  \n",
      "0     1.468232 -0.223839  \n",
      "1    -1.445272  0.703845  \n",
      "2     2.159476  2.292756  \n",
      "3    -0.357086  0.222278  \n",
      "4     0.197228  1.101067  \n",
      "...        ...       ...  \n",
      "3401 -1.658520  0.068287  \n",
      "3402 -2.378218  0.631734  \n",
      "3403 -2.219247 -1.925801  \n",
      "3404  1.718342 -2.939651  \n",
      "3405  0.695193  0.079280  \n",
      "\n",
      "[3406 rows x 9 columns]\n",
      "     ^GSPC_lag     ^IXIC      ^DJI     ^FCHI    ^GDAXI     ^AORD      ^HSI  \\\n",
      "0     0.527419  0.614642  0.234403  0.087430  0.535185 -0.643652  1.548751   \n",
      "1     0.140535  0.456396 -0.075720 -0.549652 -0.922300 -0.199854  0.126817   \n",
      "2     0.363628  0.201025  1.323524  0.824626  0.344590  0.644323  1.277591   \n",
      "3     0.203256  0.066414  0.316613 -0.348487 -0.096975 -0.088907 -0.366784   \n",
      "4     0.154447  0.443672 -0.330987 -0.647213 -0.039745 -0.116237 -1.222102   \n",
      "..         ...       ...       ...       ...       ...       ...       ...   \n",
      "786  -0.960359 -1.105417 -1.017226  0.817362  0.476524 -0.418538  0.563005   \n",
      "787   1.381262  1.137867  1.575048  0.489760  0.974486  1.684674  0.023422   \n",
      "788   0.813924  0.777104  0.480739  0.495796  0.687923  0.616985  0.091122   \n",
      "789   0.537455  0.746563 -0.404681 -0.260466  0.089372  0.229473  0.113318   \n",
      "790   1.456302  1.414844  1.190522  0.701666 -0.008597  0.459408 -0.461617   \n",
      "\n",
      "        ^N225     ^NSEI  \n",
      "0   -0.054061 -1.246090  \n",
      "1    0.202198  0.449959  \n",
      "2   -0.254789  0.211842  \n",
      "3   -0.232775 -0.689133  \n",
      "4   -0.403263 -0.381033  \n",
      "..        ...       ...  \n",
      "786  0.616445 -0.917999  \n",
      "787 -0.853382 -0.087000  \n",
      "788 -0.853382  0.255904  \n",
      "789  2.507061 -1.449764  \n",
      "790 -0.329692 -0.244761  \n",
      "\n",
      "[791 rows x 9 columns]\n",
      "torch.Size([3396, 10, 9])\n",
      "torch.Size([3396, 1])\n",
      "torch.Size([781, 10, 9])\n",
      "torch.Size([781, 1])\n"
     ]
    }
   ],
   "source": [
    "%run DataProcessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([32, 10, 9]) torch.Size([32, 1])\n",
      "torch.Size([4, 10, 9]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=50, num_layers=3, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shape of each out:\n",
    "        # (batch_size, sequence_length, hidden_size)\n",
    "        outputs, _ = self.lstm(x)\n",
    "        outputs = self.fc(outputs[:, -1, :])\n",
    "        return outputs\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss() # Mean Squared Error\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mean Absolute Percentage Error (MAPE) function\n",
    "def get_mape(y_true, y_pred):\n",
    "    if y_true.shape == y_pred.shape: \n",
    "        epsilon = 1e-8  # Small value to avoid division by zero\n",
    "        percentage_error = torch.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "        \n",
    "        # Calculate the mean of absolute percentage error\n",
    "        mape = torch.mean(percentage_error)\n",
    "        \n",
    "        return mape.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70], MSE: 1513.52, MAPE: 1.00\n",
      "Epoch [2/70], MSE: 1503.25, MAPE: 1.00\n",
      "Epoch [3/70], MSE: 1490.63, MAPE: 0.99\n",
      "Epoch [4/70], MSE: 1483.14, MAPE: 0.99\n",
      "Epoch [5/70], MSE: 1459.97, MAPE: 0.98\n",
      "Epoch [6/70], MSE: 1457.66, MAPE: 0.98\n",
      "Epoch [7/70], MSE: 1505.96, MAPE: 1.01\n",
      "Epoch [8/70], MSE: 1507.15, MAPE: 1.01\n",
      "Epoch [9/70], MSE: 1479.43, MAPE: 0.97\n",
      "Epoch [10/70], MSE: 1478.16, MAPE: 0.97\n",
      "Epoch [11/70], MSE: 1420.03, MAPE: 0.95\n",
      "Epoch [12/70], MSE: 1398.38, MAPE: 0.94\n",
      "Epoch [13/70], MSE: 1467.56, MAPE: 0.96\n",
      "Epoch [14/70], MSE: 1421.76, MAPE: 0.95\n",
      "Epoch [15/70], MSE: 1397.78, MAPE: 0.94\n",
      "Epoch [16/70], MSE: 1386.23, MAPE: 0.93\n",
      "Epoch [17/70], MSE: 1381.38, MAPE: 0.93\n",
      "Epoch [18/70], MSE: 1375.22, MAPE: 0.92\n",
      "Epoch [19/70], MSE: 1380.66, MAPE: 0.93\n",
      "Epoch [20/70], MSE: 1375.05, MAPE: 0.93\n",
      "Epoch [21/70], MSE: 1380.32, MAPE: 0.93\n",
      "Epoch [22/70], MSE: 1464.36, MAPE: 0.96\n",
      "Epoch [23/70], MSE: 1413.44, MAPE: 0.95\n",
      "Epoch [24/70], MSE: 1469.30, MAPE: 0.96\n",
      "Epoch [25/70], MSE: 1468.74, MAPE: 0.96\n",
      "Epoch [26/70], MSE: 1463.03, MAPE: 0.96\n",
      "Epoch [27/70], MSE: 1446.43, MAPE: 0.95\n",
      "Epoch [28/70], MSE: 1386.66, MAPE: 0.93\n",
      "Epoch [29/70], MSE: 1374.78, MAPE: 0.93\n",
      "Epoch [30/70], MSE: 1364.95, MAPE: 0.92\n",
      "Epoch [31/70], MSE: 1353.17, MAPE: 0.91\n",
      "Epoch [32/70], MSE: 1342.53, MAPE: 0.91\n",
      "Epoch [33/70], MSE: 1338.99, MAPE: 0.90\n",
      "Epoch [34/70], MSE: 1327.20, MAPE: 0.89\n",
      "Epoch [35/70], MSE: 1309.06, MAPE: 0.88\n",
      "Epoch [36/70], MSE: 1425.43, MAPE: 0.91\n",
      "Epoch [37/70], MSE: 1310.72, MAPE: 0.89\n",
      "Epoch [38/70], MSE: 1304.95, MAPE: 0.86\n",
      "Epoch [39/70], MSE: 1314.43, MAPE: 0.86\n",
      "Epoch [40/70], MSE: 1320.29, MAPE: 0.87\n",
      "Epoch [41/70], MSE: 1317.66, MAPE: 0.87\n",
      "Epoch [42/70], MSE: 1356.48, MAPE: 0.92\n",
      "Epoch [43/70], MSE: 1474.21, MAPE: 0.94\n",
      "Epoch [44/70], MSE: 1444.48, MAPE: 0.92\n",
      "Epoch [45/70], MSE: 1427.13, MAPE: 0.91\n",
      "Epoch [46/70], MSE: 1297.07, MAPE: 0.86\n",
      "Epoch [47/70], MSE: 1499.93, MAPE: 1.01\n",
      "Epoch [48/70], MSE: 1429.91, MAPE: 0.89\n",
      "Epoch [49/70], MSE: 1415.06, MAPE: 0.87\n",
      "Epoch [50/70], MSE: 1398.21, MAPE: 0.86\n",
      "Epoch [51/70], MSE: 1376.35, MAPE: 0.84\n",
      "Epoch [52/70], MSE: 1377.02, MAPE: 0.84\n",
      "Epoch [53/70], MSE: 1300.29, MAPE: 0.81\n",
      "Epoch [54/70], MSE: 1243.28, MAPE: 0.78\n",
      "Epoch [55/70], MSE: 1151.18, MAPE: 0.75\n",
      "Epoch [56/70], MSE: 971.38, MAPE: 0.66\n",
      "Epoch [57/70], MSE: 822.55, MAPE: 0.59\n",
      "Epoch [58/70], MSE: 755.63, MAPE: 0.55\n",
      "Epoch [59/70], MSE: 659.46, MAPE: 0.50\n",
      "Epoch [60/70], MSE: 745.51, MAPE: 0.54\n",
      "Epoch [61/70], MSE: 561.73, MAPE: 0.48\n",
      "Epoch [62/70], MSE: 529.04, MAPE: 0.48\n",
      "Epoch [63/70], MSE: 482.36, MAPE: 0.47\n",
      "Epoch [64/70], MSE: 449.41, MAPE: 0.47\n",
      "Epoch [65/70], MSE: 416.74, MAPE: 0.47\n",
      "Epoch [66/70], MSE: 398.03, MAPE: 0.46\n",
      "Epoch [67/70], MSE: 365.31, MAPE: 0.44\n",
      "Epoch [68/70], MSE: 330.75, MAPE: 0.34\n",
      "Epoch [69/70], MSE: 311.47, MAPE: 0.44\n",
      "Epoch [70/70], MSE: 273.51, MAPE: 0.34\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 70\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # (Batch Size, Time Steps, Number of Features)\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        train_predictions = model(X_batch)\n",
    "        mse = criterion(train_predictions, y_batch)\n",
    "        mse.backward()\n",
    "        optimizer.step()\n",
    "    mape = get_mape(y_batch, train_predictions)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], MSE: {mse.item():.2f}, MAPE: {mape:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 2107.05, Average MAPE: 1.51\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_mse = []\n",
    "    test_mape = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        test_predictions = model(X_batch)\n",
    "        mse = criterion(test_predictions, y_batch)\n",
    "        mape = get_mape(y_batch, test_predictions)\n",
    "        test_mse.append(mse.item())\n",
    "        test_mape.append(mape)\n",
    "    \n",
    "    average_mse_loss = sum(test_mse) / len(test_mse)\n",
    "    average_mape_loss = sum(test_mape) / len(test_mape)\n",
    "    print(f'Average MSE: {average_mse_loss:.2f}, Average MAPE: {average_mape_loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
